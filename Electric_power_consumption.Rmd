---
title: "Time Series Report (Final Exam-DSTI)"
author: "Borja Gonzalez del Regueral"
date: "10/31/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    fig_caption: yes
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```

# Introduction
A dataset containing 15 min electric power records and temperature has been provided with a time span between the 1st of January 2010 and the 17th of February 2010. The aim of this report is to provide the best possible forecast of the electric power consumption every 15 min for the 17th of February. For this, forecasting models will be trained considering and not considering outdoors temperature as an exogenous variable.
The steps that have been followed to arrive to the final two forecasts detailed in this report follow a usual time series analysis and forecasting workflow:

**Data ingestion and Preparation:** Data has been provided in an `.xlsx ` file and it has been imported into a data frame. Subsequently, data has been transformed into the adequate objects in R to be used for modelling. The formats are two first as a `ts()` and as a `tsibble()` objects. The reason behind this duality in formats is that both packages forecast and fable, feasts will be used. Although fpp3 is more flexible when dealing with sub daily frequencies. The downside is this package is that is not fully developed and stable therefore some models will be run using the forecast package (for example ARIMA).
There are no outliers and the data has a frequency of 96 as there is a daily cycle of the recorded data every 15 min. Once it has been transformed the data is analysed in an exploratory data analysis to have a better understanding of the time series.

**Exploratory Data Analysis:** The main characteristics of the time series have been analysed, cycle, trend and seasonality through graphical representation of the time series and different tests. The time series presents a small trend which increases slightly at the beginning and afterwards decreases. Seasonality appears not only on a daily basis but also sub daily with the consumption patterns during the early hours of the day (increasing consumption) stabilizing during the working hours, having an additional increase (evening) and decreasing late at night. This is a pattern that repeats every day in the building irrespective of the day being a weekday or a weekend. There is only a change in the maximum values obtained. There is no variance inestability across the time series.
The time series is split into a train and a test set having the test set the same duration as the forecasting horizon required 96 data points.

**Modelling:** Modelling has been done following a competitive strategy. As there is no way of knowing in advance which models will work better all of them have been tried. As a performance metric RMSE in the test set has been chosen. Furthermore, modelling has been split between models with and without including the exogenous variable `outdoor temperature`. 

The first models to be tried are the benchmark models (Naïve, Seasonal Naïve, Random Walk with drift) which will set the minimum RMSE that has to be beaten by any other model that will add more complexity.
    
*Models tried *without considering the outdoor temperature* are: ETS, ARIMA, Dynamic Regression, Neural Networks, Regression, STL+ETS and STL+ARIMA.
    
*Models tried *considering the outdoor temperature* are: Regression, Dynamic Regression, Neural Networks and Vector Auto-Regression models.

**Model Selection:** Models in both categories (with and without outdoor temperature) have been ranked according to the RMSE obtained on the test set and the one with the lowest RMSE in each category has been chosen for the final forecast. Although this is a deviation from the standard practice where the best model among all should be used (the one with the lowest RMSE or any other metric of choice) this is a given constraint as there is a need to provide two forecasts.

**Forecast:** The final forecasts provided by these two models are saved on a `.xlsx` file and submitted with this report.

The above-mentioned steps will be followed during the subsequent stages of the report.

# Libraries
```{r, warning= FALSE}
library("fpp2")
library("fpp3")
library("feasts")
library("ggpubr")
library("fable")
library("tsibble")
library("lubridate")
library("tidyverse")
library("readxl")
library("writexl")
library("ggfortify")
library("fabletools")
library("vars")
library("MASS")
```

# Data Import
Data will be imported into a data frame from the original `.xlsx` dataset.
```{r pressure, echo=FALSE}
#Set up the working environment and read the file
setwd("/Users/borja_mini/Documents/Time Series")
data <- read_excel("Elec-train.xlsx")
```

Once the dataset is imported into a dataframe an initial inspection of the data types, missing values, etc. will be done that will determine the next steps regarding data preparation.
```{r}
#Convert the file into a dataframe and check the main characteristics
df<-data.frame(data)
#Check the type of variables in the dataframe after importing it
str(df)
#Get a summary to know the minimum and maximum of each variable
summary(df)
#Get a first glimpse of the data
head(df, 10)
tail(df, 10)
```

The dataset contains data on the power consumption and temperature recorded every 15 minutes from the 1/1/2010 until the 17/2/2010. As the forecast needs to be done for the 17/2/2010 the power consumption in kW has not been included. Temperature has been given until the end of the period which will be used as exogenous variable for covariant forecasting.

# Data Preparation

From a data quality perspective `Timestamp` has been imported as `chr` and requires to be transformed into a date-time variable and both `Power..kW.`and `Temp..C.` are double-precision `dbl` which will remain. Furthermore, the data needs to be transformed into a time series format. In this case a `tsibble` format will be used.
```{r}
#Transform the timestamp from chr to a data time variable
df_2<-data.frame(interval =seq(as.POSIXct("2010/01/01 01:15", tz="GMT"), 
                    as.POSIXct("2010/02/17 23:45", tz="GMT"),
                    by='15 min'), df)
#Keep the relevant variables only
keep<-c("interval", "Power..kW.", "Temp..C..")
df_3<-df_2[keep]
#Drop the interval variable and rename to Timestamp
df_4<-rename(Timestamp = interval, df_3)
#Check the new data
str(df_4)
```

Once all the data has been adequately cleaned and transformed, it has to be transformed into a time series object to be able to work with time series libraries.
```{r}
#Transform the data into a tsibble object to work with it
electricity<-as_tsibble(df_4, index = Timestamp)
#Check the first rows of the time series.
head(electricity, 10)
```

There is no data for the last day 17/2/2010 which will be forecasted so we will coerce the time series to consider only the working data. The starting point of the working data will be the first timestamp point `2010-01-01 01:15:00` and the last one `2010-02-16 23:45:00`.Data for 17/2/2020 will remain unused until the final forecasting models are selected.
```{r}
#Filter the gaps to leave a clean dataset for model selection
electricity_work<-filter_at(electricity, vars(Power..kW.) , any_vars(! is.na(.)))

#Structure of the time series
str(electricity_work)

#Print tail to check that everything works
tail(electricity_work, 10)

#Boxplot for temperature and power
boxplot(electricity_work["Power..kW."])
boxplot(electricity_work["Temp..C.."])
```
The data will also be prepared for the final forecast when considering the outdoors temperature containing only the data for 2/17/2010:
```{r}
#Leave a clean dataset for final forecast with only the 2/17/2010
electricity_forecast<-filter_at(electricity, vars(Power..kW.) , any_vars(is.na(.)))
```

Data has been cleaned and transformed into a time series object `tsibble`. The missing values to forecast have been removed in both `Power..kW.` and `Temp..C..` so that a final cleaned working dataset can be used for model selection and as an input for forecasting purposes. Boxplots of both time series will be obtained to see iof there are outliers. In this case the target time series is the `Power..kW.` which is the one we want to forecast on. `Power..kW.` has no outliers and `Temp..C..` present two outliers at the end of the period under analysis. As some packages are still not stable in `fpp3` the working data will also be transformed into a `ts` object.
```{r}
#Transform the data into a ts object to work with it
electricity_ts<-as.ts(electricity_work, frequency = 96)
#Check the first rows of the time series.
head(electricity_ts, 10)
```

# Exploratory Data Analysis (EDA)

Trend, seasonality and cyclicity will be analysed before proceeding to model selection. This will improve the understanding of the dataset and the different periodicities that might appear in the `Power..kW.` series. The exploratory data analysis will not consider missing points as this has been already covered in the previous steps. The basic features of the time series:
```{r}
#Summary of the time series
summary(electricity_work)
```

The median value of `Power..kW.` is slightly higher than the mean as it happend in `Temp..C..` with the same values. The minimum value happened early in the morning of the first day and the highes value in the last datapoint recorded. The same happends with the temperature so it suggests that both are moving in the same direction but further correlation analysis will be required for this. Regarding the `Power..kW.` time series different plots will be obtained to have a better understanding of its behaviour:
```{r}
#Plot of the time series
autoplot(electricity_work, .vars = Power..kW.)
#Plot temperatures
autoplot(electricity_work, .vars = Temp..C..)
#Plot of the time series in a monthly basis
gg_season(electricity_work, Power..kW., period = "month")
#Plot of the time series to analyse weekday vs. weekend
gg_season(electricity_work, Power..kW., period = "week")
#Plot of the time series with a period of day
gg_season(electricity_work, Power..kW., period = "day")
```

Graphically, the time series seems to shows no trend (which will require further analysis) and accused seasonality. Within the week, we can see how it repeats every 96 recorded points (per day) the same pattern. Low power demand during the early hour of the day that continuously increase until 7:00 am with a high peack when work starts and continues until 18:00 (most probably during working hours or central hours of the day during weekends). At 18:00 power demand increases until 300 kW and slowly decreases until 22:00 where it experiences a sharp decrease until 23:59. hence, the time series presents seasonality with a frequency of 96 recorded points which is equivalent to daily seasonality. This pattern repeats during the eight weeks in which data has been recorded. During the first weeks (January) there is a significant increase of power demand in the evening (from 18:00 onwards) which is due to the temperature decrease experienced during the same period.

A lag plot is produced to see if lags could be used as predictors
```{r}
##Lags plot ot see if lags can be used as predictors
gg_lag(electricity_work, Power..kW., period = 96, lags = 1:9, geom = "point", arrow = TRUE) +
  labs(x = "lag(Power, kW)")
```

It can be seen that all lags have a strong an positive relationship when considering central hours which is reduced if the first hours of the day are cpnsidered. The same happens with the late evening (from 22:00 onwards).

The temperature time series will also be analysed to see if there is a similar pattern and correlation when it is used to forecast `Power..kW.`
```{r}
#Plot scatter plot between temperature and Power consumption
ggplot(electricity_work, aes(x= Temp..C.., y = Power..kW.))+ 
  geom_point()+
  labs(x = "Temperature (C)", y = "Power (kW)")

#Plot scatter plot between temperature and Power consumption
ggplot(electricity_work, aes(x= I(Temp..C..)^2, y = Power..kW.))+ 
  geom_point()+
  labs(x = "Temperature^2 (C)", y = "Power (kW)")
```

There is a relationship between two levels of power and temperature. The first one gravitates around 175kW for temperatures between 5C and 15C while the second one is higher 275kW with temperatures between 7C and 17.5C. This can be due to the differences of temperature between day and night. During the day electricity consumption is higher as the building is occupied and there are appliances working while during the night electricity consumption is lower as it was seen before. The same relationship is maintained with the square of the Temperature against Power.

```{r}
autoplot(ACF(electricity_work, Power..kW., lag_max = 300))
autoplot(PACF(electricity_work, Power..kW., lag_max = 200))
```

The shape of the ACf suggests that there might be a trend as the peask and troughs of the function decrease over lags. Furthermore, it can be seen that there is seasonality on the time series. It could be hypothesized that it follows an Ar(2) model dur to the shape, typlical for this type of models. The PACF shows the there is a clear seasonality at lag 96 and multiples of it as we saw before. The difference between the PACF and the ACF points out the amount of information that is contained in the intermediate lags suggesting that a lagged prediction could be used to model the time series. To have a better understanding of the trend, seasonality and residuals the time series will be decomposed.

## Trend and seasonality decomposition
As it is electricity consumption there will be different components of the seasonality that has been previously seen. The trend and seasonality decomposition using Loess will be applied. Arguments in the function for the trend and season windows will be chosen to try to get the maximum information from the time series.
```{r}
#STL decomposition and values of the diffeerent components
electricity_decomp<-model(electricity_work, STL(Power..kW.~ trend(window = 53, degree = 1) +
                                                  season(period = "day", window = 7)+
                                                  season(period = "week", window = 7)+
                                                  season(period = 96/2, window = 7)
                                                  , robust = TRUE))

#Plot the decomposition to see the different components
autoplot(components(electricity_decomp))
#Residuals of the decomposition
gg_tsresiduals(electricity_decomp, lag_max = 96)
#Acf and Pacf for residuals
autoplot(ACF(components(electricity_decomp), remainder))
autoplot(PACF(components(electricity_decomp), remainder))
```

The time series has multiple seasonalities. The first one is the daily seasonality identified earlier which is determined by the consumption and the daytime and nighttime during the day. There is also a week seasonality which has a trough on Sundays due to the decrease of consumption in the building as there might be less activity. This is complemented by an additional seasonality that represents a half a day seasonality. I this case the residuals are not white noise and they are not normally distributed but could be modeled following an Ma or an ARMA model that will be done later. The t.window chosen is slightly higher than the midday in 15 min periods which will avoid taking part of the seasonality. It shows a downwards trend which means that the consumption is lower in time as temperature in February increases. It shows a significant peak on day 12 as the temperature experiences a significant trough.
Residuals show that there is no seasonality left through the Pacf and ACF although there is a trend in the first 11 lags. Residuals could be modeled using an ARMA(9,11) or ARMA(9,13) according to Pacf and Acf.
Regarding the variance stability, a BoxCox transformation will be done to see the effect on the time series and lambda will be obtained.
```{r}
#Calculate lambda using BoxCox transformation according to Guerrero
lambda<-features(electricity_work, Power..kW., features = guerrero)
#Lambda value
lambda
#Plot the transformed time series
autoplot(electricity_work, box_cox(Power..kW., lambda))
```

The value of lambda is 0.66 and at first sight there is not a lot of change on the time series from the plot above. The transformed and non transformed time series will be used to reach the best possible accuracy on residuals.

# Accuracy Measure

To compare models in a competitive way RMSE root mean squared error will be used trying to minize the value of it while having white noise on residuals.

# Split the dataset into train and test sets

The dataset is split into a train and a test set. The length of the test set is equivalent to the length of the forecast that has to be provided, 96 points. The forecasting horizon h will be set to 96 and models will be evaluated once they are trained on the test set. The best univariant and covariant models will be considered and predictions will be made with both of them.
```{r}
#Forecasting horizon
h=96
#Train set using slice
train<-slice(electricity_work, 1:(n()-h))
#Print the tail of the training set
head(train,10)
#test set with 96 observations as the forecast that has to be provided
test<-slice(electricity_work, (n()-h+1):n())
#Print the test set
head(test, 10)
#Check the length of the train set
str(train)
#Check the length of the test set
str(test)
autoplot(train, Power..kW.)
```

From the original `train` and `test`sets `Power..kW.`and `Temp..C..` will be split into two different time series to have the training and testing sets for each of them
```{r}
#Electricity train set
electricity.train<-train[,1:2]
#Test set with 96 observations as the forecast that has to be provided
electricity.test<-test[,1:2]
#Temperature train and test sets
cols<-c(1,3)
temperature.train<-train[,cols]
temperature.test<-test[,cols]
```

The same split will be done with the `ts` object so that the most convenient package will be used according to the needs of the project. Both the previously calculated train and test sets and the ones calculated now are equal in values and size so that they can be used indistinctly.
```{r}
#Split time series into train and test
train_ts<-subset(electricity_ts, start = 0, end = length(electricity_ts[,1])-h)
test_ts<-subset(electricity_ts, start = length(electricity_ts[,1])-h+1)
#Electricity train and test set as ts()
electricity.train_ts<-train_ts[,1]
electricity.test_ts<-test_ts[,1]
#Temperature train and test set as ts()
temperature.train_ts<-train_ts[,2]
temperature.test_ts<-test_ts[,2]
```

# Modelling
## Benchmark models

For the forecasting process we are trying to find in a competitive way the best possible forecast for a day or a horizon of 96 data points to be forecasted.The best benchmark model (lowest error) when forecasting will be the threshold or benchmark for subsequent models that will be more complex. The bechnmark models that are going to be used to establish the threshold are:

-**Naive**: where the values of the forecast are equal to the value of the last observation.
-**Seasonal naive**: which will be more adequate in this case due to the high seasonality of this time series. In this case the season is considered when a forecast is done in a similar way to the Naive model.
-**Random walk with drift**: where the forecast increases or decreases with time depending on the drift of the time series but it doens't consider seasonality which will penalize its accuracy

Every subsequent model should improve the accuracy pf the best benchmark model. To measure the accuracy of the forecast RMSE (root mean squared error) will be used.
```{r}
#Fit the benchmark models
benchmarks <- model(
  electricity.train,
  "Mean" = MEAN(Power..kW.),
  "Naive" = NAIVE(Power..kW.),
  "Seasonal_Naive" = SNAIVE(Power..kW.~ lag(96)),
  "Random_Walk_w_Drift" = RW(Power..kW.~drift())
)

#Forecast on the test set with the benchmark models
benchmarks_fc<-forecast(benchmarks, h = 96) 

#Accuracy of the models (RMSE only)
RMSE_benchmark<-accuracy(benchmarks_fc, electricity.test)[,c(1,4)]
RMSE_benchmark

#Plot the benchmark methods
autoplot(benchmarks_fc, electricity.test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

As expected the best benchmark model is the Seasonal naive as it is taking into consideration the seasonality of the time series. The RMSE of the model (Seasonal naive) is 19.78 which will be used as a threshold. The plot of the test set against the benchmark models forecast highlights that the `Seasonal_Naive` model is the one that better forecasts in the test set. This procedure will be followed for the rest of the models although in this case only the `Seasonal_Naive` model will be tested as it is the best one.

```{r warning=FALSE}
#BoxCox test to see the autocorrelations in the residuals.
features(
  dplyr::select(
    augment(
      dplyr::select(benchmarks, Seasonal_Naive)),
    Timestamp,
    .innov), 
  .innov, ljung_box)

#Check residuals of the model
gg_tsresiduals(dplyr::select(benchmarks, Seasonal_Naive))

```

It can be seen from the ACF plot that there is still information within the residuals as there are spikes passing the threshold which means that the residuals are not white noise. From the histogram it can be seen that residuals are not normally distributed although the mean is zero. From the `Ljung-Box` test, pvalue<0.5 therefore we reject the null hypotehsis and there are autocorrelations in the residuals. This means, that tere is room for improvement in the model as there is information that has been left in the residuals.

## Exponential Smoothing (ETS)

From the exponential smoothing models available only those with seasonality and trend will be considered as the time series shows a clear seasonality and a decreasing trend during the period. Hence, the Holt Winters seasonal method with additive and multiplicative seasonality will be considered.
```{r}
#Create a function to fit both models
hw_add_fit = HoltWinters(electricity.train_ts, seasonal = "additive")
hw_mult_fit = HoltWinters(electricity.train_ts, seasonal = "multiplicative")
```

As the function `ets() & hw()`  and `ETS()` in the `fpp2` and `fpp3`only allows seasonality under 24, the `HoltWinters` function from `stats` package has been used to fit and train the abovementioned models. In this case, the use of any of the former would bring and artificial increase of RMSE since the state space models will be correctly chosen but by design the function will ignore a seasonality of period = 96 (without warnings unless it is explicitly called). The later will provide the proper RMSE considering the period of this time series.
Regarding the quality of the fit, in the Holt-Winters additivie method:
```{r}
#Test if the residuals of the fitted function present autocorrelation
Box.test(hw_add_fit$x, lag = 10,type = "Lj")
#Test if the residuals of the fit are normally distributed
shapiro.test(residuals(hw_add_fit))
#Check residuals on the fit of the additive Holt-Winters model
checkresiduals(hw_add_fit)
```

For the `Holt-Winters additive` model, residuals are not white noise therefore there is still information within them so the model can be improved. Furthermore, the mean of the residuals is zero but they are not normally distributed which will bring narrower predictive intervals.
```{r}
#Test if the residuals of the fitted function present autocorrelation
Box.test(hw_mult_fit$x, lag = 10,type = "Lj")
#Test if the residuals of the fit are normally distributed
shapiro.test(residuals(hw_mult_fit))
#Check residuals on the fit of the multiplicative Holt-Winters model
checkresiduals(hw_mult_fit)
```

The `Holt-Winters multiplicative` model does not present white noise in the residuals although the spikes are smaller than in the case of the additive version of the model. As in the previous case, residuals have zero mean but are not normally distributed, therefore narrower predictive intervals are expected. Forecasting both methods and checking on the accuracy of the forecast:
```{r}
#Forecast on the test set the exponential smoothing models previously fitted
hw_add_fc<-forecast(hw_add_fit, h = 96) 
hw_mult_fc<-forecast(hw_mult_fit, h = 96)

#Accuracy of the models
RMSE_additive<-accuracy(hw_add_fc, electricity.test_ts)[2,2]
RMSE_multiplicative<-accuracy(hw_mult_fc, electricity.test_ts)[2,2]

#Show accuracy of the models
Models = c('Holt Winters Additive', 'Holt-Winters Multiplicative')
RMSE = round(c(RMSE_additive,RMSE_multiplicative), 2)
cbind(Models, RMSE)
```

```{r}
#Plot the holt winters models
autoplot(hw_add_fc)
```

## ARIMA and SARIMA models

To fit the ARIMA models first the time series will be checked to see if there is any differencing required. In this case according to the Agumented Dickey-Fuller test there is no differencing required to stabilise the trend making the time series stationary. Kwiatkowski-Phillips-Schmidt-Shin test shows that no differencing is required but the strength of the sesonality requires one seasonal differencing.
```{r}
#Augmented Dickey Fuller Test
ndiffs(electricity.train_ts, test = "adf")
#KPSS test to check differencing
ndiffs(electricity.train_ts, test = "kpss")
#Seasonality test to check differencing
nsdiffs(electricity.train_ts, test = "seas")
```

The time series will be differenced once to remove the seasonality that is not making it stationary according to the "seas" test.
```{r}
#Differencing on the seasonality of the time series
electricity.train_ts_diff<-diff(electricity.train_ts, lag = 96)
#Plot the differenced time series
ggtsdisplay(electricity.train_ts_diff)

#Plot the ACF and PACF graphs for the first 3*96 lags (three days)
ggAcf(electricity.train_ts_diff, 3*96)
ggPacf(electricity.train_ts_diff, 3*96)

#Plot the ACF and PACF graphs for the first 96 lags
ggAcf(electricity.train_ts_diff, 20)
ggPacf(electricity.train_ts_diff, 20)
```

The ACF and PACF that includes three days shows that there is seasonality on the partial autocorrelations graph. In this case, an seasonal AR(3) could capture the information about the seasonality of the time series. on the correlogram that includes three days we can see that there is a significan spike in the second seasonal lag so a seasonal MA(2) could be considered.
Outside of the seasonality, from the correlogram it can be observed decreasing spikes up until lag = 13 (exponentially). Although st first sight a MA(13) could be used, it seems to be too high for an ARIMA model and the number of components could significantly penalise feature selection. Furthermore, dynamic regression could be used or dynamic harmonic regression could be used leaving the ARIMA models just for the residuals. From the PACF the fifth spike is the last significant one which will bring an AR(5). The `auto.arima` function will find the best suitable candidate for the dataset considering that it is not stationary (requires differencing) as previousy seen and that one will be improved manually.
```{r}
auto_arima_fit<-auto.arima(electricity.train_ts,
                           max.p = 5, #max order of the p values in Arima
                           max.q = 8, #Max order of q values in Arima
                           approximation = T, #Allow the approximation and re-fitting to happen
                           stepwise = T, #Search in a stepwise way for the best model
                           trace = T, #Show the models as they are fitted
                           ic = "aicc", #AICC method for model selection
                           test = "adf", #Test stationarity using the Augmented Dickey-Fuller test
                           seasonal = T, #The time series is seasonal
                           seasonal.test = "seas", #Check for seasonality
                           stationary = F, #The time series (original) is not stationarity forcing the function to difference
                           nmodels = 100 #Max number of models to be tried
                           )
```

From and auto ARIMA perspective, the model selected is ARIMA(5,0,0)(0,1,0)[96] according to the AICC criteria. Due to the restrictions imposed to the function and the approximations this might not be the best possible model. For this residuals must be checked to see oif there is still information within them. From the initial analysis it seems that the seasonal component is small so not all the seasonality has been camptures and the seasonal component should be increased. 
```{r}
fit_auto<-Arima(electricity.train_ts, order = c(5,0,0), seasonal = c(0,1,0))
#Check residuals
checkresiduals(fit_auto)
#Shapiro test for normality on residuals
shapiro.test(fit_auto$residuals)
#Plot residuals
ggtsdisplay(residuals(fit_auto))
```

There is still information in the residuals as they are not white noise. Furthermore, they are not normally distributed so the predictive intervals will be narrower than expected. There is a spike at lag 96 and 192 (ACF) that have not been captured by the model as they are significant and appears in the residuals. From the visual inspection done before on the ACF and PACf several candidates will be fit improving the results obtaioned by the `auto.arima` function.
```{r}
#Candidate 1
(fit1<-Arima(electricity.train_ts, order = c(5,0,0), seasonal = c(0,1,2)))
#Candidate 2
(fit2<-Arima(electricity.train_ts, order = c(5,0,0), seasonal = c(0,1,3)))
#Candidate 3
(fit3<-Arima(electricity.train_ts, order = c(5,0,0), seasonal = c(0,1,1), lambda = "auto"))
```

The best model from the candidates is ARIMA(5,0,0)(0,1,2)[96] as adding an additional Q order to the seasonal part of the model is not improving its AICC. There is still seasonality left on the model. Furthermore, there are coefficients (SMA2 and SMA3) that are not significant in the model. Models ARIMA(5,0,0)(3,1,1)[96] and ARIMA(5,0,0)(3,1,2)[96] that could fit as potential models from the residuals of the best potential candidate are unstable and cannot be fitted. Checking the residuals of the best candidate:
```{r}
#Check residuals
checkresiduals(fit1)
#Shapiro test for normality on residuals
shapiro.test(fit1$residuals)
#Plot residuals
ggtsdisplay(residuals(fit1))
```

Although there are some lags that remain significant, its value is lower (by ten times) so it has captured some of the information kept in the residuals but not all. Residuals have zero mean but according to the Box Cox test, there is still autocorrelation in the residuals and they are not normally distributed which will narrow the confidence interval of the predictions. The chosen candidate will be tested and accuracy obtained as it is the best one from the residuals and AICC perspective.
The third model presents a lower AICC but as the train data has been transformed through the BoxCox transformation AICCs are not comparable.
```{r}
#Check residuals
checkresiduals(fit3)
#Shapiro test for normality on residuals
shapiro.test(fit3$residuals)
#Plot residuals
ggtsdisplay(residuals(fit3))
```

The three models will be used to forecast (autoarima, the best candidate and the one using the BoxCox transformation). It will check on the test set which one provides a better forecast on the test set. This is due to the fact that the third model cannot be compared as it is fitted in a different base due to the BoxCox transformation and AICC cannot be used as a measure of fit.
```{r}
#Forecast with three candidates
fit_auto_arima_fc<-forecast(fit_auto, h = h)
fit1_fc<-forecast(fit1, h = h)
fit3_fc<-forecast(fit3, h = h)

#Accuracy of the models
RMSE_auto_arima<-accuracy(fit_auto_arima_fc, electricity.test_ts)[2,2]
RMSE_best_candidate<-accuracy(fit1_fc, electricity.test_ts)[2,2]
RMSE_fit3<-accuracy(fit3_fc, electricity.test_ts)[2,2]

#Show accuracy of the models
Models = c('ARIMA(5,0,0)(0,1,0)[96]', 'ARIMA(5,0,0)(0,1,2)[96]', 'ARIMA(5,0,0)(0,1,1)[96] w/ BoxCox transformation')
RMSE = round(c(RMSE_auto_arima,RMSE_best_candidate, RMSE_fit3), 2)
cbind(Models, RMSE)
```

From a forecasting perspective the best possible model is the one provided by the `auto.arima` function although the best candidate and the one given by the function are very close to each other in terms of RMSE. In any case, both models have an RMSE that is bigger than the one provided by the `Seasonal Niave` model so they are not good candidates for the final forecast as they are more complex and less accurate. Nevertheless, the forecast of the best model will be checked on the time series
```{r}
#Plot forecast
autoplot(fit_auto_arima_fc)
```

## Neural Networks

A neural network will be fit considering the seasonal component of the time series. A feed-forward neural netowrk will be fit uswing lagged inputs for forecasting purposes
```{r}
#Fit the model
(nn_fit<-nnetar(electricity.train_ts))

#Check residual,
checkresiduals(nn_fit)
#Check normality of the residuals
shapiro.test(nn_fit$residuals)
#Check residuals lags
ggtsdisplay(residuals(nn_fit))
```

From the residuals it can be see that there is information still contained in the residuals. To check its accuracy, the fitted model will be used for forecasting a horizon of h = 96 and the accuracy in terms of RMSE will be calculated:
```{r}
#Forecast with the neural network
nn_fc<-forecast(nn_fit, h = h)
#Accuracy of the model
RMSE_nn_fc<-accuracy(nn_fc,electricity.test_ts)[2,2]

#Show accuracy of the model
Model = c('Neural Networks')
RMSE = round(c(RMSE_nn_fc), 2)
cbind(Model, RMSE)
```

The neural network model is providing the best accuracy on the test set. Up until now, the neural network is the best candidate for forecasting as it is the one with the lowest RMSE on the test set.

## Regression (including Harmonic Regression)

Using the trend and the seasonality of the time series, it will be fitted and the model used for forecasting purposes using regression. For this, three relationships with the trend will be used, linear, quadratic and cubic. Furthermore, the seasonality will also be modeled using fourier (harmonic regression) to try to improve the model.
```{r}

#Regression based on trend and dummy seasonality
regression_fit<-model(
  electricity.train,
  "linear" = TSLM(Power..kW.~trend() + season(period = 96)),
  "Seasonality only" = TSLM(Power..kW.~season(period = 96)),
  "quadratic" = TSLM(Power..kW.~poly(trend(),2) + season(period = 96)),
  "cubic" = TSLM(Power..kW.~poly(trend(),3) + season(period = 96)),
  "fourier" = TSLM(Power..kW. ~fourier(period = 96, K=27))
)

#Check AICC of the models
glance(regression_fit)[,c(1,10)]
```

All the models present a similar AICC. In this case the difference is very small but above the threshold. In any case, all will be used to forecast and the RMSE calculated to choose the best regression model.
```{r}
#Forecast on the test set the regression models
regression_fc<-forecast(regression_fit, h = h) 

#Accuracy of the models (RMSE only)
RMSE_regression<-accuracy(regression_fc, electricity.test)[,c(1,4)]
RMSE_regression

#Plot the regression models
autoplot(regression_fc, electricity.test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

The model `Seasonal only` and `fourier` (harmonic regression) with a period = 96 similar accuracy. In this case, both are the best models obtained so far (in addition to the `linear` model) which suggests that Dynamic Regression will be a good candidate for forecasting, although it cannot be known until the model is fitted, a forecast is provided and the errors tested on the test set. Hence, from a regression perspective, the linear model is the best one. From a forecasting perspective, the `Seasonality only` model is the best regression model.

## Dynamic Regression

From the previous results, it can be observed that regression is for the time being giving the best accuracy. In the case of dynamic regression, the purpose of the model is to fit the regression (either with the trend, with seasonality or harmonic (fourier)) and model the residuals (which will not be white noise) with an ARIMA model.
```{r}
dynamic_regression_fit<-model(
  electricity.train,
  "dynamic_det_trend" = ARIMA(Power..kW.~0+trend()+pdq(d = 0)),
  "dynamic_det_season" = ARIMA(Power..kW.~0+season(period = 96)+PDQ(0,0,0)),
  "dynamic_harmonic" = ARIMA(Power..kW.~0+fourier(period = 96, K = 25) + PDQ(0,0,0))
)
```

From the models that have been fitted, the coefficients will be unpacked to know the ARIMA model that has been used for the residuals.
```{r}

#Coefficients of the model: dynamic_det_trend
report(dplyr::select(dynamic_regression_fit, dynamic_det_trend))

#Coefficients of the model: dynamic_det_season
coefficients(dplyr::select(dynamic_regression_fit, dynamic_det_season))[1:6,c(1,2)]

#Coefficients of the model: dynamic_harmonic
coefficients(dplyr::select(dynamic_regression_fit, dynamic_harmonic))[1:5,c(1,2)]
```

The `dynamic_det_trend` fits the trend and an ARIMA(3,0,1), the `dynamic_det_season` fits the seasonality and uses an ARIMA(2,0,4) and the `dynamic_harmonic` an ARIMA(0,0,5) after fitting the fourier series. The autocorrelation on the residuals will be tested using a portmanteau test (Ljung test) and residuals will be plotted:
```{r}
#BoxCox test to see the autocorrelations in the residuals: dynamic_det_season
features(
  dplyr::select(
    augment(
      dplyr::select(dynamic_regression_fit, dynamic_det_season)),
        Timestamp,
    .innov),
  .innov, ljung_box)

#Check residuals of the model: dynamic_det_season
gg_tsresiduals(dplyr::select(dynamic_regression_fit, dynamic_det_season))

#BoxCox test to see the autocorrelations in the residuals: dynamic_harmonic
features(
  dplyr::select(
    augment(
      dplyr::select(dynamic_regression_fit, dynamic_harmonic)),
    Timestamp,
    .innov), 
  .innov, ljung_box)

#Check residuals of the model: dynamic_harmonic
gg_tsresiduals(dplyr::select(dynamic_regression_fit, dynamic_harmonic))
```

Both the `dynamic_det_season` and the `dynamic_harmonic` have the residuals as white noise meaning that they have been able to capture all the information on the time series when fitting. The accuracy of the forecast for the three models will be tested and the three models will be plotted on the test set.
```{r}
#Forecast on the test set the dynamic regression models
dynamic_regression_fc<-forecast(dynamic_regression_fit, h = h) 

#Accuracy of the models (RMSE only)
RMSE_dynamic_regression<-accuracy(dynamic_regression_fc, electricity.test)[,c(1,4)]
RMSE_dynamic_regression

#Plot the regression models
autoplot(dynamic_regression_fc, electricity.test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

From a competitive perspective the model with the lowest RMSE is the `dynamic_det_season` which makes it a good candidate for the final model selection. This models are aligned with the ones obtained with regression although there is a significant improvement due to the use of an ARIMA model for the residuals.

## STL and ETS, ARIMA

### STL + ETS

To deal with the complexities of the seasonality of this time series, an STL decomposition is coupled with an ETS and an ARIMA model. In the first case, the use of STL overcomes the limitation of frequency <24 imposed to the ETS() function due to the restrictions and complexity of finding the initial states. Although ARIMA can deal with frequencies up to 350 it is more rigid than a hybrid model. The strategy that will be followed will consist on leaving some of the seasonality on the trend so that the seasonally adjusted component includes some of that seasonality,
```{r}
#Fit the model, first the decomposition to deal with the high frequency seasonality and afterwards the ETS()
stl_ets_fit <-model( 
  electricity.train,
  "stl_ets" = decomposition_model(
  STL(Power..kW.~season(period = 96), robust = TRUE),
  ETS(season_adjust~season("N"))
))

#Report the model
report(stl_ets_fit,stl_ets)
```

Once the model is fit, after the STL decomposition, the seasonal adjusted data is modeled using an ETS(A,N,N) or a  simple exponential smoothing model. Autocorrelation on the residuals and their behaviour will be analysed.
```{r}
#BoxCox test to see the autocorrelations in the residuals: stl_ets_fit
features(
  dplyr::select(
    augment(stl_ets_fit),
    Timestamp,
    .innov), 
  .innov, ljung_box)

#Check residuals of the model: stl_ets_fit
gg_tsresiduals(stl_ets_fit)
```

The residuals are white noise as the p-value is higher than 0.05 so it fails to reject the null hypothesis being the residuals white noise but not normally distributed so narrower predictivie confidence intervales will be obtained. 
```{r}
#Forecast on the test set the STL+ETS
stl_ets_fc<-forecast(stl_ets_fit, h = h) 

#Accuracy of the model (RMSE only)
RMSE_stl_ets<-accuracy(stl_ets_fc, electricity.test)[,c(1,4)]
RMSE_stl_ets

#Plot the STL+ETS
autoplot(stl_ets_fc, electricity.test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

The accuracy of the forecast measured on RMSE is similar to the obe obtained with dynamic regression models although it is better than the `Holt-Winters` models that have been tested before.

### STL + ARIMA

In this case an STL decomposition will be coupled with an ARIMA model. The strategy to follow will be similar to the one used for ETS. Some seasonality will remain on the trend so that the seasonaly adjusted component captures it and can be modelled through ARIMA.
```{r}
#Fit the model, first the decomposition to deal with the high frequency seasonality and afterwards the ETS()
stl_arima_fit <-model( 
  electricity.train,
  "stl_arima" = decomposition_model(
  STL(Power..kW.~season(period = 96), robust = TRUE),
  ARIMA(season_adjust)
))
#Report the model
report(stl_arima_fit,stl_arima)
```

Once the model is fitted, after the STL, the sesonal adjusted data is modeled with an ARIMA(2,1,4). This model will be analysed to understand their behaviour and if all the information on the tie series has been captured by the model.
```{r}
#BoxCox test to see the autocorrelations in the residuals: stl_arima_fit
features(
  dplyr::select(
    augment(stl_arima_fit),
    Timestamp,
    .innov), 
  .innov, ljung_box)

#Check residuals of the model: stl_arima_fit
gg_tsresiduals(stl_arima_fit)
```

Residuals are white noise as there is no autocorrelation according to the Ljung test and the mean is zero. They are not normally distributed so they are not gaussian white noise. The model will be used to forecast on the test set and the accuracy of the forecast will be obtained.
```{r}
#Forecast on the test set the STL+ARIMA
stl_arima_fc<-forecast(stl_arima_fit, h = h) 

#Accuracy of the model (RMSE only)
RMSE_stl_arima<-accuracy(stl_arima_fc, electricity.test)[,c(1,4)]
RMSE_stl_arima

#Plot the STL+ARIMA model on the test set
autoplot(stl_arima_fc, electricity.test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

# Models Using Outdoors Temperature

The second part of this assignment will be dealing with similar models using exogenous variables. In this case, the exogenous variable to be used has been provided in the original dataset `Temp..C..`. Values are also given for the final forecast that meeds to be provided.

## Regression with exogenous factors

Regression models previously used will be trained on the dataset considering `Temp..C..` as an exogenous variables. The purpose of the use of this variable is to improve the results previously obtained. In this case temperature and the square of temperature will be used as exogenous variables.
```{r}
#Regression based on trend and dummy seasonality
regression_temp_fit<-model(
  train,
  "linear_temp" = TSLM(Power..kW.~Temp..C..+I(Temp..C..^2)+trend()),
  "Seasonality_temp" = TSLM(Power..kW.~Temp..C..+I(Temp..C..^2)+season(period = 96)),
  "fourier" = TSLM(Power..kW. ~Temp..C..+I(Temp..C..^2)+fourier(period = 96, K=27))
)

#Check AICC of the models
glance(regression_temp_fit)[,c(1,10)]

#Check residuals of the model: Seasonality_temp_only
gg_tsresiduals(dplyr::select(regression_temp_fit, Seasonality_temp))
```

The best model from an AICC perspective is the `Seasonality_temp` which includes the first two powers of temperature and the seasonality with a frequency of 96. Although this should be the model that goes into the next phase the three are going to be tested on the test set to compare the RMSE obtained by all of them. It can be onserved in the residuals that information is still kept and that the model could be improved. Furthermore, residuals are not normally distributed so they are not Gaussian and nor white noise.  Only residuals for the `Seasonality_temp` model are displayed as all of the models follow the same pattern.
The test set for temperature will be prepared and the models fit on the test set.
```{r}
#Prepare new data for the forecasting period
temp_test<- mutate(new_data(train, 96), Temp..C.. = test$Temp..C..)

#Forecast on the test set the regression models
regression_temp_fc<-forecast(regression_temp_fit, new_data = temp_test) 

#Accuracy of the models (RMSE only)
RMSE_temp_regression<-accuracy(regression_temp_fc, test)[,c(1,4)]
RMSE_temp_regression

#Plot the regression models
autoplot(regression_temp_fc, test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

The best model from a forecasting perspective is the `Seasonality_temp` as is the one with the lowest RMSE. Further improvement for the model could be done considering an ARIMA model capturing the information on the residuals.

## Dynamic Regression with exogenous factors

Following the same approach as with the regression models, for the dynamic regression models an exogenous predictor will be used `Temp..C..`. One of the models to be tested and improved will be the one previously obtained `Seasonality_temp` as and ARIMA model in the errors could capture part of the information left behind from the linear regression model. The evolution of this model through dynamic regression is the `dynamic_temp2_season` model.
```{r}
dynamic_regression_temp_fit<-model(
                                  train,
  "Dynamic_temp" = ARIMA(Power..kW.~0+Temp..C..
                         +PDQ(P = 0:2, D = 0:1, Q = 0:2, period = 96)),
  "Dynamic_temp2" = ARIMA(Power..kW.~0+Temp..C..+I(Temp..C..^2)
                          +PDQ(P = 0:2, D = 0:1, Q = 0:2, period = 96)),
  "Dynamic_temp2_season" = ARIMA(Power..kW.~0+Temp..C..+I(Temp..C..^2)+season(period = 96)
                                +PDQ(0,0,0))
)
```

Once the models have been trained with the exogenous regressor, the coefficients for the ARIMA models on the residuals are obtained and plotted to analyse their behaviour.
```{r}
#Coefficients of the model
report(dplyr::select(dynamic_regression_temp_fit,Dynamic_temp))
#Residuals of the model
gg_tsresiduals(dplyr::select(dynamic_regression_temp_fit,Dynamic_temp))

#Coefficients of the model
report(dplyr::select(dynamic_regression_temp_fit,Dynamic_temp2))
#Residuals of the model
gg_tsresiduals(dplyr::select(dynamic_regression_temp_fit,Dynamic_temp2))

#Coefficients of the model
coefficients(dplyr::select(dynamic_regression_temp_fit,Dynamic_temp2_season))
#Residuals of the model
gg_tsresiduals(dplyr::select(dynamic_regression_temp_fit,Dynamic_temp2_season))
```

All the models significantly improve the behaviors of the residuals once they are trained. The ARIMA model on residuals helps in all cases to capture more information from the residuals. Although they are still not white noise, the significant spikes have decreased in value around 10 times having less information kept in the autocorrelations. During the fitting, the seasonality of the time series has been captured through the regression and only the non-seasonal correlations have been captured by the ARIMA model. The nonexistence of white noise in the residuals means that there might be some seasonality that has moved into the residuals. Nevertheless, this has been tested providing also SARIMA model for the residuals but the forecasting performance has been worsen.
From the residuals perspective, the `Dynamic_Temp2_seaon` is an ARMA(2,4) which is aligned with the behavior of the residuals shown when fitting the `Seasonality_temp`.
Once the exogenous factor is prepared for forecasting, a forecast is carried out on the test set to check its accuracy through RMSE.
```{r}
#Prepare new data for the forecasting period
temp_test<- mutate(new_data(train, 96), Temp..C.. = test$Temp..C..)

#Forecast on the test set
dynamic_regression_temp_fc<-forecast(dynamic_regression_temp_fit, new_data = temp_test) 

#Accuracy of the models (RMSE only)
RMSE_dynamic_regression_temp_fc<-accuracy(dynamic_regression_temp_fc, electricity.test)[,c(1,4)]
RMSE_dynamic_regression_temp_fc

#Plot the models on the test set
autoplot(dynamic_regression_temp_fc, electricity.test) +
  facet_grid(rows = vars(.model), scales = "free_y") + 
  guides(color = "none") +
  ylab("Power..kW")
```

Although the RMSE was expected to be lowered during the fit of the ARIMA model on the error of the regression, it has increased and has made the model more complex. In the case of the `Dynamic_temp2_season`, compared to the `Seasonality_temp` the increase of 0.5 points which could be considered as a null contribution of the ARIMA model on the errors of the regression model.

## Neural Networks with exogenous predictor
As with previous models, the Neural Network will be trained using an external regressor. The initial conditions are the same than the conditions previously used but an additional regressor is included in the model.
```{r}
#Fit the model
nn_temp_fit<-nnetar(train_ts[,1], xreg = train_ts[,2]+I(train_ts[,2]^2))

#Check residual,
checkresiduals(nn_temp_fit)

#Check normality of the residuals
shapiro.test(nn_temp_fit$residuals)

#Check residuals lags
ggtsdisplay(residuals(nn_temp_fit))
```

Once the model is trained, it can be seen that is not capturing seasonality, which raises the question of using a Fourier or seasonality() series as a regressor to help the model to get more information from the time series leaving less in the residuals. Once again the residuals are not white noise and are not normally distributed. Different configurations of the neural network have been tried but in any of the cases white noise has been achieved.
Although white noise has not been obtained on the residuals, the model is used to provide a forecast:
```{r}
#Forecast with the neural network
nn_temp_fc<-forecast(nn_temp_fit, xreg = test_ts[,2])
#Accuracy of the model
RMSE_nn_temp_fc<-accuracy(nn_temp_fc,test_ts[,1])[2,2]

#Show accuracy of the model
Model = c('Neural Networks')
RMSE = round(c(RMSE_nn_temp_fc), 2)
cbind(Model, RMSE)
```

The introduction of the predictor worsens the performance of the model increasing its RMSE on the test set to values similar to the one of the best benchmark model `Seasonal_Naive`. This model doesn't show the performance required to be a good candidate for the final forecasting task.

## VAR: Vector Auto-Regression
In this case a bidirectional relationship is assumed in which power consumption is influenced by temperature and viceversa as predictors are influenced by forecasted variables and the other way around. Both variables are treated as endogenous in the model. As the time series are not stationary the first thing to do is to check the  differences that would apply to use the season component in the VAR model. Due to the strong seasonality of the time series, seasonal difference will be applied first and if needed additional non-seasonal differences will be applied.
```{r}
#Number of differences required to make Temp..C.. stationary
nsdiffs(train_ts[,"Temp..C.."])

#Number of differences required to make Powe..kW. stationary
nsdiffs(train_ts[,"Power..kW."])

#Difference both time series
train_ts_diff<-diff(train_ts, lag = 96)

#Check if additional differences is required
ndiffs(train_ts_diff[,"Temp..C.."])
ndiffs(train_ts_diff[,"Power..kW."])
```

Both time series are not stationary and present strong seasonality, hecen the VAR model and the selection of variables (lags) will include the season to reduce the number of lags required. Due to the frequency of the time series 96, a season with that frequency will be introduced and multiples of 96 will be used to choose the lags. The best number of lags will be found form 1 until the max lag provided 96. It has been tested with values up to 120 and the niminum RMSE is given with the selected value equal to 96.

```{r}
#Set of values for the maximum lag over which the function will iterate
windows <- expand.grid(initial_lags = seq(1, 110, 1), nb_lags=NA, RMSE_var_fc = NA)

#Iterate through all the values
for(row.i in 1:nrow(windows)) {
  
  #Set the lags to the initial value 
  initial_lags.i <- windows$initial_lags[row.i]
  
  #Find the elected max.lags to be used by the VAR model
  nb_lags.i<-VARselect(train_ts, lag.max = initial_lags.i, type = "const")
  #Train and fit the model
  var_fit<-VAR(train_ts, p = nb_lags.i$selection[1], type = "const")
  #Forecast using the trained model
  var_fc<-forecast(var_fit, h = 96)
  #Calculate the error on the test set measured by RMSE
  RMSE_var_fc.i<-accuracy(var_fc$forecast$Power..kW., test_ts[,"Power..kW."])[2,2]
  #Populate the table   
  windows$initial_lags[row.i] <- initial_lags.i
  windows$nb_lags[row.i] <- nb_lags.i$selection[1]
  windows$RMSE_var_fc[row.i] <- RMSE_var_fc.i
}

#Plot the results to find the minimum
plot(windows$initial_lags, windows$RMSE_var_fc )

#Print the results
tail(windows,20)
```

Once the best value is found, the final model will be trained using the selected number of lags and test the residuals to see if there is information remaining on the residuals. The minimum value of the selected lags thar provide the lowest RMSE is 96.
```{r}
#Fit the model using the number of lags
var_fit<-VAR(train_ts, p = 96, type = "const")

#Check autocorrelations on residual
residuals_autocorr<-serial.test(var_fit, lags.pt = 96,  type = "PT.asymptotic")
#Print residuals
residuals_autocorr

#Check for normality of residuals
residuals_norm <- normality.test(var_fit, multivariate.only = TRUE)
#Print residuals
residuals_norm
```

Residuals on the fitted value are not normal so the predictive intervals will be narrower than expected although they have mean zero. The fitted model will be used to forecast on the test set and the RMSE will be calculated
```{r}
#Forecast with the VAR
var_fc<-forecast(var_fit, h = h)

#Calculate Accuracy of the model for Power..kW.
RMSE_var_fc<-accuracy(var_fc$forecast$Power..kW., test_ts[,"Power..kW."])[2,2]

#Show accuracy of the model
Model = c('VAR')
RMSE = round(c(RMSE_var_fc), 2)
cbind(Model, RMSE)
```

Plot the forecast of both time series with the model that consideres the bidirectional relationship between both.
```{r}
#Plot the forecast of both time series
autoplot(var_fc)
```

The minimum RMSE is lower than the best benchmark model but is not better than the regression models. This can be due to the lack of flexibility of the model compared to the regression one where the seasonality is captured either through harmonic regression or through the seasonality of the time series. It is a good candidate but not the best one for the final forecast.

# Model selection

All models have been fitted and tested considering the RMSE in the test set as the main measure of forecasting power. The aim of using this measure is to rank them from the lowest to the highest RMSE. This will give the best and worst model of all the models that have been used. Those models that present an RMSE higher than the best benchmark model are automatically discarded as they will add complexity but will not significantly improve their forecasting performace. Models with and without an exogenous regressor have been fitted and will be ranked within their category and overall.

The models ranked in an increasing RMSE (best to worst performance) **not using any exogenous predictor**:
```{r}
#Names of the models
models<-c("Dynamic Regression with Season", "Regression with Seasonality", "Harmonic Dynamic Regression", "STL+ETS", "Regression with Fourier Series",
          "STL+ARIMA", "Regression with Linear Trend", "Holt Winters Multiplicative", "Neural Networks", "Holt Winters Additive", "Seasonal Naive",
          "ARIMA(5,0,0)(0,1,0)[96]", "ARIMA(5,0,0)(0,1,2)[96]", "ARIMA(5,0,0)(0,1,2)[96] w/ BoxCox transformation", "Regression with Quadratic Trend",
          "Dynamic Regression with deterministic trend", "Regression with Cubic Trend", "Benchmark-Mean", "Random Walk with Drift", "Naïve")

#RMSE of the models
RMSE<-c(14.99, 15.00, 15.35, 15.45, 15.49, 15.67, 17.37, 17.67,
        18.01, 19.16, 19.78, 20.00, 20.02, 20.11, 36.50, 39.23,
        40.40, 60.84, 89.81, 89.83)

#Build dataframe
model_ranking<-data.frame(models, RMSE)

#Print the dataframe
model_ranking
```
**The best model** is the `Dynamic Regression model using a season` of period = 96 which is more or less equivalent to the regression model using seasonality. As the criterion has been set to the minimum RMSE although the difference is very small this is the model that will be used to provide the first forecast (the one without considering the outdoors temperature). All models that are ranked after the Seasonal Naive model are automatically discarded as they are more complex than a simple benchmark model and have a lower accuracy (higher RMSE on the test set). Hence, all models from 12-20 should be automatically discarded.

The models ranked in an increasing RMSE (best to worst performance) **using outdoors temperature as a predictor**:
```{r}
#Models with outdoors temperature
models_reg<-c("Regression with Temperature Squared and Season", "Dynamic Regression with Temp squared and Season",
              "Regression with Temp Squared and Fourier", "VAR(96)", "Neural Network with Temp Squared", "Dynamic Regression with Temp",
              "Dynamic Regression with Temp Squared", "Regression with Temp Squared") 

#Models accuracy
RMSE_reg<-c(14.38, 14.83, 14.88, 18.28, 18.57, 20.02, 20.19, 50.38)

#Build dataframe
model_ranking_with_outdoors_temp<-data.frame(models_reg, RMSE_reg)

#Print dataframe
model_ranking_with_outdoors_temp
```
From the models that have been trained and tested using the outdoors temperature, the `Regression with Temperature Squared and Season` is the best model and will be the model used to provide the second forecast of this assignment. As in the previous case, all models with a higher RMSE than 19.78 (corresponding to the `Seasonal Naive` model) are automatically discarded as they perform worse than a benchmark model being more complex. Hence, all the models that adding an external predictor are not including any seasonal modelling.

From all the models the best model is the one that uses outdoors temperature as an external predictor `Regression with Temperature Squared and Season` performing better than any of the models that are not incorporating this predictor. There is a 4.1% increase in performance decreasing the RMSE from 14.99 (`Dynamic Regression model using a season`) to 14.83 (`Regression with Temperature Squared and Season`).

# Forecasts (and save to a .xlsx file)

## Model not including outdoor temperature

The best model **not including outdoors temperature** is `Dynamic Regression model using a season` which will be used to provide the final forecast after being trained in the whole dataset
```{r}
#Train the best model on the whole dataset
Dynamic_Regression<-model(
  electricity_work,
  "Dynamic_Regression_with_Season" = ARIMA(Power..kW.~0+season(period = 96)+PDQ(0,0,0))
)
```

Once the model is trained on the whole dataset it will be used to forecast a horizon of 96 datapoints which is equivalent to the whole 2/17/2010
```{r}
#Final Forecast for the 2/17/2010
Dynamic_Regression_fc<-forecast(Dynamic_Regression, h = h) 

#Plot the forecast
autoplot(Dynamic_Regression_fc)
```

## Model including outdoor temperature

The best model **including outdoors temperature** is `Dynamic Regression model using a season` which will be used to provide the final forecast after being trained in the whole dataset
```{r}
#Train the best model on the whole dataset
Dynamic_Regression_Temperature<-model(
  electricity_work,
  "Regression_with_Temperature_Squared_and_Season" = ARIMA(Power..kW.~0+Temp..C..+I(Temp..C..^2)+season(period = 96)+PDQ(0,0,0))
)
```

Once the model is trained on the whole dataset it will be used to forecast a horizon of 96 datapoints which is equivalent to the whole 2/17/2010
```{r}
#Prepare new data for the forecasting period
temp_final<- mutate(new_data(electricity_work, h), Temp..C.. = electricity_forecast$Temp..C..)

#Forecast on the test set
Dynamic_Regression_Temperature_fc<-forecast(Dynamic_Regression_Temperature, new_data = temp_final) 

#Plot the forecast
autoplot(Dynamic_Regression_Temperature_fc)
```

## Export results into an `.xlsx` file

Create the final tibble with the point forecasts for 17/2/2010
```{r}
#Access the Timestamp and the final forecast converting it into a tibble for both forecasts
Forecast_NOT_incl_Outdoor_Temperature<-dplyr::select(as_tibble(Dynamic_Regression_fc), Timestamp, .mean)
Forecast_incl_Outdoor_Temperature<-dplyr::select(as_tibble(Dynamic_Regression_Temperature_fc), Timestamp, .mean)

#Left join forecasts using the Timestamp
join_forecasts<-left_join(Forecast_NOT_incl_Outdoor_Temperature, Forecast_incl_Outdoor_Temperature, by = "Timestamp")

#Rename the columns and drop the timestamp
renamed_tibble<-rename(join_forecasts, Forecast_NOT_incl_Outdoor_Temperature=.mean.x, Forecast_incl_Outdoor_Temperature=.mean.y)
final_forecasts<-dplyr::select(renamed_tibble,c("Forecast_NOT_incl_Outdoor_Temperature", "Forecast_incl_Outdoor_Temperature"))

#Check that the final tibble meets the requirements (96 rows and 2 columns)
final_forecasts
```

Export the tibble into a `.xlsx` file
```{r}
#Write the file with the name and no headers in th3 working directory
write_xlsx(final_forecasts,"Borja_Gonzalez_del_Regueral.xlsx", col_names = FALSE)
```

# Conclusions

In this assignment we have been given a dataset containing 15 min complete data of Power consumption in (kW) and Temperature in (Celsius) starting on the 1st of January 2010 and finishing on the 16th of February 2010. The aim of the assignment is to forecast Power consumption for every 15 min on the 17th of February 2010. The original dataset did not have any missing values or outliers although it presented complex seasonality.

An initial analysis of the time series has been done to determine its trend and seasonality. It presents a slight trend and multiple seasonality. The most accused one is the daily seasonality although it also has sub-daily seasonality consisting of the Power consumption on the morning, working hours and evening. It presents a peak on Friday compared to the rest of the days and a small decrease during peak hours on Thursdays across the period under analysis. There is a difference in consumption with a decrease of the peaks and increase of the troughs before and after week three.

To forecast the final period the time series has been split into a train and test set having the later the same size as the period for which the final forecast will be provided (h = 96 data points). Models with and without considering outdoor temperature have been trained and tested in a competitive way. A total of 28 models (20 without considering the temperature and the remainder taking it into account) have been tested using RMSE on the test set as the ranking metric. Models have been ranked according to their forecasting performance (from lowest to highest RMSE). 

The best two models (lowest RMSE in their category) have been `Dynamic Regression model using a season`  and `Regression with Temperature Squared and Season`.  Nevertheless, the later using outdoors temperature as an exogenous predictor has the best accuracy in terms of RMSE as expected.

Once the best two models have been selected, both have been trained on the whole dataset and the final two forecasts have been done. Following the instructions of the assignment the final forecasts have been exported into an `.xlsx` file.

